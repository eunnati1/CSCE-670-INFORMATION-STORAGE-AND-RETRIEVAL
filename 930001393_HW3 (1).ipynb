{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 3:   Recommender System Practice: Rating Prediction and Top-K Item Recommendation\n",
    "\n",
    "### 100 points [ 6% of your final grade]\n",
    "\n",
    "### Due: April 10, 2020\n",
    "\n",
    "*Goals of this homework:* Understand matrix factorization (MF) using explicit feedback and Bayesian Personalized Ranking (BPR) using implicit feedback for recommendation. Explore different methods for two real-world recommendation senarios: rating prediction and top-K item recommendation.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw3.ipynb`. For example, my homework submission would be something like `555001234_hw3.ipynb`. Submit this notebook via eCampus (look for the homework 3 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the total late days you have remaining).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Matrix Factorization for Rating Prediction (70 points total)\n",
    "\n",
    "In some platforms, such as MovieLens, users express their preference on items using explict feedback like ratings.\n",
    "\n",
    "In this part, you will implement matrix factorization to predict ratings on MovieLens data. After removing users who left less than 20 ratings and movies with less than 20 ratings, the provided dataset has only ~1,200 items and ~500 users. You can also check the title and genres of each movie in *movies_info.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Load the Data (5 points)\n",
    "\n",
    "Please download the dataset from Piazza. There are about 65,000 ratings in total. We split the rating data into two sets. You will train with 70% of the data (in *train_movie.csv*) and test on the remaining 30% of data (in *test_movie.csv*). Each of train and test files has lines having this format: UserID, MovieID, Rating. \n",
    "\n",
    "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "import surprise\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRAIN DATA \n",
      "==========================\n",
      "   userId  movieId  rating\n",
      "0       0        4       4\n",
      "1       0       23       5\n",
      "2       0       25       5\n",
      "3       0       31       3\n",
      "4       0       33       5\n",
      "\n",
      "         TEST DATA \n",
      "==========================\n",
      "   userId  movieId  rating\n",
      "0       0      635       4\n",
      "1       0      587       5\n",
      "2       0      339       5\n",
      "3       0      250       5\n",
      "4       0      411       5\n",
      "\n",
      "Number of unique users in Train Data: 541\n",
      "Number of unique movies in Train Data: 1211\n",
      "\n",
      "Number of unique users in Test Data : 541\n",
      "Number of unique movies in Test Data : 1211\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of\n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "\n",
    "trainset = pd.read_csv('train_movie.csv')\n",
    "print(\"         TRAIN DATA \")\n",
    "print(\"==========================\")\n",
    "print(trainset.head())\n",
    "print()\n",
    "print(\"         TEST DATA \")\n",
    "print(\"==========================\")\n",
    "testset = pd.read_csv('test_movie.csv')\n",
    "print(testset.head())\n",
    "\n",
    "unique_users_trainset =  len(trainset.userId.unique())\n",
    "unique_movies_trainset = len(trainset.movieId.unique())\n",
    "\n",
    "unique_users_testset =  len(testset.userId.unique())\n",
    "unique_movies_testset = len(testset.movieId.unique())\n",
    "\n",
    "print()\n",
    "print('Number of unique users in Train Data:', unique_users_trainset )\n",
    "print('Number of unique movies in Train Data:', unique_movies_trainset )\n",
    "print()\n",
    "print('Number of unique users in Test Data :', unique_users_testset )\n",
    "print('Number of unique movies in Test Data :', unique_movies_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: Matrix Factorization (40 points)\n",
    "\n",
    "In class, we introduced how matrix factorization works for recommendation. Now it is your term to implement it. There are different methods to obtain the latent factor matrices **P** and **Q**, like gradient descent, Alternating Least Squares (ALS), and so on. Pick one of them and implement your MF model. *You can refer to tutorials and resources online. Remember our **collaboration policy** and you need to inform us of the resources you refer to.* \n",
    "\n",
    "Please report MAE and RMSE of your MF model for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = pd.pivot_table(trainset,values='rating',index=['movieId'],columns=['userId'],fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "R = np.array(rating_matrix)\n",
    "N = len(R)  # 1211\n",
    "M = len(R[0]) #541\n",
    "\n",
    "U = np.random.rand(N,K)\n",
    "V = np.random.rand(M,K)\n",
    "iterations = 80\n",
    "alpha = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MF_Gradient_Descent(R,U,V,K,iterations,alpha):\n",
    "    V = V.T \n",
    "    for it in range(iterations):  \n",
    "        for i in range(len(R)): \n",
    "            for j in range(len(R[i])):  \n",
    "                if R[i][j] > 0:\n",
    "                    err = R[i][j] - np.dot(U[i,:],V[:,j])  #find error\n",
    "                    \n",
    "                    for k in range(K):\n",
    "                        U[i][k] = U[i][k] + alpha * (2 * err * V[k][j])\n",
    "                        V[k][j] = V[k][j] + alpha * (2 * err * U[i][k])\n",
    "        R1 = np.dot(U,V)\n",
    "        error = 0\n",
    "        for x in range(len(R)):\n",
    "            for y in range(len(R[x])):\n",
    "                if R[x][y] > 0:\n",
    "                    error = error + pow(R[x][y] - np.dot(U[x,:],V[:,y]), 2)\n",
    "                    for z in range(K):\n",
    "                        error = error + ( pow(U[x][z],2) + pow(V[z][y],2) )\n",
    "        if error < 0.001:\n",
    "            break                \n",
    "    return U, V.T                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ui , Vi = MF_Gradient_Descent(R,U, V, K,iterations,alpha)\n",
    "predicted_matrix = np.dot(Ui,Vi.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(testset):\n",
    "    predicted_results, actual_results = [], []\n",
    "    for col,row in testset.iterrows():\n",
    "        actual_results.append(row['rating'])\n",
    "        predicted_results.append(predicted_matrix[row['movieId']][row['userId']])\n",
    "    return actual_results,predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATION OF RMSE AND MAE OF MODEL FOR TEST SET\n",
    "def calculate_RMSE(original_rating, predicted_rating):\n",
    "    err = 0\n",
    "    total_ratings = len(original_rating)\n",
    "    for i in range(0,total_ratings):\n",
    "        err = err + math.pow((original_rating[i] - predicted_rating[i]),2)\n",
    "    err = err / len(original_rating)\n",
    "    rmse = math.sqrt(err)\n",
    "    return rmse\n",
    "\n",
    "def calculate_MAE(original_rating, predicted_rating):\n",
    "    err = 0\n",
    "    total_ratings = len(original_rating)\n",
    "    for i in range(0,total_ratings):\n",
    "        err = err + abs((original_rating[i] - predicted_rating[i]))\n",
    "    mae = err /  total_ratings    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Matrix Factorization Using Gradient Descent\n",
      "==================================================\n",
      "The Mean Absolute Error for Test Data is : 0.6940211809384695\n",
      "The Root Mean Squared Error for Test Data is : 0.9027726341425492\n"
     ]
    }
   ],
   "source": [
    "MF_actual_ratings,MF_predicted_ratings = predictions(testset)\n",
    "\n",
    "print('  Matrix Factorization Using Gradient Descent')\n",
    "print(\"==================================================\")\n",
    "print('The Mean Absolute Error for Test Data is :', calculate_MAE(MF_actual_ratings,MF_predicted_ratings))\n",
    "print('The Root Mean Squared Error for Test Data is :', calculate_RMSE(MF_actual_ratings,MF_predicted_ratings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method did you use to obtain **P** and **Q**? What are the advantages and disadvantages of the method you pick? *provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent training algorithm was used to compute P and Q (latent features) which are Ui and Vi in my code.It is a convex optimisation method. How far we are at one time (prediction) tells us how to improve prediction and which direction to go.This is done by intializing the two matrices with some values, calculate how different their product is to R (finding error) and then try to minimize this difference iteratively.This method aims at finding a local minimum of the difference**\n",
    "\n",
    "**Advantages include its cost effectiveness, computational efficency and that it produces a stable error gradient and a stable convergence. It is computationally fast because it doesnt require any computation of second-derivatives i.e only one sample is processed at a time thus making it give speedier results.It is easier to fit into memory due to a single training sample being processed by the network. For larger datasets it can converge faster as it causes updates to the parameters more frequently.**\n",
    "\n",
    "**Some disadvantages are that the stable error gradient can sometimes result in a state of convergence that isn't the best the model can achieve.The learning rate can affect which minimum we reach and how quickly we reach it.If the learning rate for gradient descent is too fast, we are going to skip the true local minimum to optimize for time. If it is too slow, the gradient descent may never converge because it is trying really hard to exactly find a local minimum.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1c: Improve MF (25 points)\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach. Hints: You may consider using the title or genres information, trying other algorithms, designing a hybrid system or considering a neighborhood like this paper [Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf). *You can do anything you like to improve MAE and RMSE.*\n",
    "\n",
    "You will get full marks for this part if you get better results than your MF results (of course we will also judge whether what you do here is reasonable or not). You will get partial marks for a reasonable effort even if you do not improve your MF results. Additionally, you will get 5 points as bonus if your model performs the best among the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularisation_term  = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MF_Gradient_Descent_Improvement(R,U,V,K,iterations,alpha,beta):\n",
    "    V = V.T \n",
    "    for it in range(iterations):  \n",
    "        for i in range(len(R)):  \n",
    "            for j in range(len(R[i])): \n",
    "                if R[i][j] > 0:\n",
    "                    err = R[i][j] - np.dot(U[i,:],V[:,j])  \n",
    "                    \n",
    "                    for k in range(K):\n",
    "                        U[i][k] = U[i][k] + alpha * (2 * err * V[k][j] - beta * U[i][k])\n",
    "                        V[k][j] = V[k][j] + alpha * (2 * err * U[i][k] - beta * V[k][j])\n",
    "        R1 = np.dot(U,V)\n",
    "        error = 0\n",
    "        for x in range(len(R)):\n",
    "            for y in range(len(R[x])):\n",
    "                if R[x][y] > 0:\n",
    "                    error = error + pow(R[x][y] - np.dot(U[x,:],V[:,y]), 2)\n",
    "                    for z in range(K):\n",
    "                        error = error + (beta/2) * ( pow(U[x][z],2) + pow(V[z][y],2) )\n",
    "        if error < 0.001:\n",
    "            break                \n",
    "    return U, V.T                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Q = MF_Gradient_Descent_Improvement(R,U, V, K,iterations,alpha,regularisation_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_matrix_impr = np.dot(P,Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_imp(testset):\n",
    "    predicted_results, actual_results = [], []\n",
    "    for col,row in testset.iterrows():\n",
    "        actual_results.append(row['rating'])\n",
    "        predicted_results.append(predicted_matrix_impr[row['movieId']][row['userId']])\n",
    "    return actual_results,predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matrix Factorization with regularisation term included to handle overfitting\n",
      "=================================================================================\n",
      "The Mean Absolute Error for test data is : 0.6804033486887275\n",
      "The Root Mean Squared Error for test data is : 0.887131899302025\n"
     ]
    }
   ],
   "source": [
    "MF_actual_ratings_imp,MF_predicted_ratings_imp = predictions_imp(testset)\n",
    "\n",
    "print(' Matrix Factorization with regularisation term included to handle overfitting')\n",
    "print(\"=================================================================================\")\n",
    "print('The Mean Absolute Error for test data is :', calculate_MAE(MF_actual_ratings_imp,MF_predicted_ratings_imp))\n",
    "print('The Root Mean Squared Error for test data is :', calculate_RMSE(MF_actual_ratings_imp,MF_predicted_ratings_imp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain what you do to improve the recommendation in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A common extension to this basic MF algorithm is to introduce regularization to avoid overfitting. This is done by adding a parameter beta and modifing the squared error as implemented in the code above.We can observe the RMSE and MAE values have reduced for the MF implementation including regularisation factor thus indicating improvement in the basic algorithm. Regularization helps to choose preferred model complexity, so that model is better at predicting. Regularization is nothing but adding a penalty term to the objective function and control the model complexity using that penalty term. It attempts to reduce the variance of the estimator by simplifying it, something that will increase the bias, in such a way that the expected error decreases.In other words, the new parameter beta introduced is used to control the magnitudes of the user-feature and playlist-feature vectors such that P (Latent vector) and Q (Latent vector) would give a good approximation of R (Rating matrix) without having to contain large numbers. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Bayesian Personalized Ranking (BPR) for Top-K Item Recommendation (30 points)\n",
    "\n",
    "Compared with rating prediction in part 1, a more popular scenario recently is personalized top-K item ranking for each user based on the user's implicit feedback. Examples include ranking videos on YouTube and ranking products on Aamzon. In practice, users tend to provide implicit feedback (e.g., the user clicked a product URL on Amazon or played a video on YouTube) rather than explicit feedback (e.g., ratings or reviews) in most cases.\n",
    "\n",
    "In this part, you will experiment with Bayesian Personalized Ranking (BPR) to rank items on a [Spotify Playlist Recommendation Dataset](http://people.tamu.edu/~yunhe/pubs/AttListCIKM2019.pdf). If a user ever followed a playlist, this interaction is treated as an implicit feedback. In our sampled dataset, there are ~10,000 users and ~7,000 playlists.\n",
    "\n",
    "BPR can generate scores of items for each user. You should rank all items based on the scores for each user and evaluate the ranking performance.\n",
    "\n",
    "For example, if user 0 has two interacted playlists 23, 78 in test.txt. If the top-10 playlists for user 0 returned by BPR is [12,45,78,34,23,90,134,33,46,9], then the precision@10 for user 0 is 0.2 because the two playlists in test.txt are recommended in top-10: 2/10=0.2. Please report NDCG@10 in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Please download the dataset from Piazza. There are about 90,000 interactions in total, which are split into training.txt, validation.txt and text.txt. You will train on train.txt, tune hyperparameters on validation.txt and report final result on test.txt in terms of NDCG@10. \n",
    "\n",
    "Each of the train and test files has lines having this format: UserID, PlaylistID, 1.0. \n",
    "\n",
    "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eunna\\Anaconda3\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import math\n",
    "from functools import reduce\n",
    "from scipy.sparse import coo_matrix\n",
    "import surprise\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from statistics import mean \n",
    "from lightfm import LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define file names\n",
    "\n",
    "#Train files\n",
    "train_txt = \"train.txt\"\n",
    "train_csv = \"trainc.csv\"\n",
    "\n",
    "#Test Files\n",
    "test_txt = \"test.txt\"\n",
    "test_csv = \"testc.csv\"\n",
    "\n",
    "\n",
    "#Validation Files\n",
    "vali_txt = \"validation.txt\"\n",
    "vali_csv = \"validationc.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(inputf,outputf):\n",
    "    \n",
    "    rows=['userId','PlaylistId',1] \n",
    "    \n",
    "    with open(outputf, 'w+') as csvFile:\n",
    "        writer = csv.writer(csvFile) \n",
    "        writer.writerow(rows)\n",
    "        \n",
    "        for f in open(inputf,\"r\"): \n",
    "            rowval=[]\n",
    "            values=f.split()\n",
    "            r=0;\n",
    "            \n",
    "            for i in range(len(rows)): \n",
    "                \n",
    "                rowval.append(values[r])\n",
    "                r += 1 \n",
    "                \n",
    "           \n",
    "            writer.writerow(rowval)\n",
    "            \n",
    "    csvFile.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "parse_data(train_txt,train_csv)\n",
    "parse_data(test_txt,test_csv)\n",
    "parse_data(vali_txt,vali_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRAIN DATA \n",
      "==========================\n",
      "   userId  PlaylistId    1\n",
      "0    3480         194  1.0\n",
      "1    3480         657  1.0\n",
      "2    3480         190  1.0\n",
      "3    3480         660  1.0\n",
      "4    3480        1437  1.0\n",
      "\n",
      "         TEST DATA \n",
      "==========================\n",
      "   userId  PlaylistId    1\n",
      "0    5988         189  1.0\n",
      "1    5988         105  1.0\n",
      "2    5988        1400  1.0\n",
      "3    5982        3589  1.0\n",
      "4    5982        4259  1.0\n",
      "\n",
      "     VALIDATION DATA \n",
      "==========================\n",
      "   userId  PlaylistId    1\n",
      "0    5988         124  1.0\n",
      "1    5988        1878  1.0\n",
      "2    5989        3418  1.0\n",
      "3    5983        1379  1.0\n",
      "4    5981        1037  1.0\n",
      "\n",
      "Number of unique users in Train Data: 10183\n",
      "Number of unique playlist in Train Data: 7787\n",
      "\n",
      "Number of unique users in Test Data : 5846\n",
      "Number of unique playlist in Test Data : 3604\n",
      "\n",
      "Number of unique users in Validation Data : 5895\n",
      "Number of unique playlist in Validation Data : 3674\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of\n",
    "# playlists and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "train_data = pd.read_csv(train_csv)\n",
    "print(\"         TRAIN DATA \")\n",
    "print(\"==========================\")\n",
    "print(train_data.head())\n",
    "print()\n",
    "print(\"         TEST DATA \")\n",
    "print(\"==========================\")\n",
    "test_data = pd.read_csv(test_csv)\n",
    "print(test_data.head())\n",
    "print()\n",
    "print(\"     VALIDATION DATA \")\n",
    "print(\"==========================\")\n",
    "vali_data = pd.read_csv(vali_csv)\n",
    "print(vali_data.head())\n",
    "\n",
    "unique_users_train_data =  len(train_data.userId.unique())\n",
    "unique_playlist_train_data = len(train_data.PlaylistId.unique())\n",
    "\n",
    "unique_users_test_data =  len(test_data.userId.unique())\n",
    "unique_playlist_test_data = len(test_data.PlaylistId.unique())\n",
    "\n",
    "unique_users_vali_data =  len(vali_data.userId.unique())\n",
    "unique_playlist_vali_data = len(vali_data.PlaylistId.unique())\n",
    "\n",
    "print()\n",
    "print('Number of unique users in Train Data:', unique_users_train_data )\n",
    "print('Number of unique playlist in Train Data:', unique_playlist_train_data )\n",
    "print()\n",
    "print('Number of unique users in Test Data :', unique_users_test_data )\n",
    "print('Number of unique playlist in Test Data :', unique_playlist_test_data)\n",
    "print()\n",
    "print('Number of unique users in Validation Data :', unique_users_vali_data )\n",
    "print('Number of unique playlist in Validation Data :', unique_playlist_vali_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR by Using Package\n",
    "\n",
    "Compared with MF, BPR is more complicated to implement. In this part, you can use a BPR package to experiment with top-K item recommendation. Some good packages include https://github.com/benfred/implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to call other BPR packages for top-K recommendation.\n",
    "# Report average NDCG@10 for all users on test.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchData(filename):\n",
    "    f = open(filename, encoding = \"utf8\")      \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_attributes(filename):\n",
    "    f = FetchData(filename)\n",
    "    uid = []\n",
    "    pid = []\n",
    "    rid = []\n",
    "    for line in f:\n",
    "        users = line.split('\\t')[0]\n",
    "        playlists = line.split('\\t')[1]\n",
    "        ratings = line.split('\\t')[2]\n",
    "        uid.append(users)\n",
    "        pid.append(playlists)\n",
    "        rid.append(ratings)\n",
    "    return uid,pid,rid    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "def form_coo_matrix(data,rows,cols):\n",
    "    matrix = coo_matrix((data, (rows, cols)))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(prob_rel,orginal_rel):\n",
    "    count = 0\n",
    "    DCG = 0\n",
    "    IDCG = 0  \n",
    "    iteration_len =len(prob_rel)\n",
    "    for i in range(iteration_len):\n",
    "        DCG += ((2**prob_rel[i])-1)/(math.log(1+i+1))\n",
    "        IDCG += ((2**orginal_rel[i])-1)/(math.log(1+i+1))    \n",
    "    if IDCG == 0:\n",
    "        NDCG = 0\n",
    "    else:\n",
    "        NDCG = (DCG/IDCG)\n",
    "    return NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_LighFM(matrix_val,playlists,users_list,model):\n",
    "    \n",
    "    scores = []\n",
    "    cal_ndcg = []\n",
    "    ndcg = 0\n",
    "    for uid in users_list:\n",
    "        playlists_ratings = matrix_val.tocsr()[uid].indices  \n",
    "        actual_list = [0,0,0,0,0,0,0,0,0,0]\n",
    "        index = 0\n",
    "        min_len = min(10, len(playlists_ratings))\n",
    "        for i in range (1, min_len + 1):\n",
    "            actual_list[index] = matrix_val.tocsr()[uid, playlists_ratings[i-1]]\n",
    "            index = index +1    \n",
    "        scores = model.predict(uid,np.arange(len(playlists)))\n",
    "        top_items = np.argsort(-scores)\n",
    "        predicted_list = []\n",
    "        for i in range (1, 11):\n",
    "            if top_items[i-1] in playlists_ratings:\n",
    "                predicted_list.append(1)\n",
    "            else:\n",
    "                predicted_list.append(0)\n",
    "       \n",
    "        cal_ndcg.append(NDCG(predicted_list,actual_list))\n",
    "    output = mean(cal_ndcg)\n",
    "              \n",
    "    return output     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned parameters\n",
    "NUM_THREADS = 2\n",
    "NUM_COMPONENTS = 50\n",
    "USER_ALPHA = 5e-10\n",
    "ITEM_ALPHA = 1e-4\n",
    "LEARNING_RATE = 0.09\n",
    "LEARNING_SCHEDULE = 'adadelta'\n",
    "RANDOM_SEED = 29031994  \n",
    "MAX_SAMPLED = 100\n",
    "RHO =0.1\n",
    "K_M = 50\n",
    "N_M = 1\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_train, pid_train, rid_train = fetch_attributes(\"train.txt\")\n",
    "uid_list = np.array(uid_train).astype(np.int)\n",
    "pid_list = np.array(pid_train).astype(np.int)\n",
    "rid_list = np.array(rid_train).astype(np.float)\n",
    "unique_user_ids = list(dict.fromkeys(uid_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = form_coo_matrix(rid_list,uid_list,pid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_vali, pid_vali, rid_vali = fetch_attributes(\"validation.txt\")\n",
    "vuid = np.array(uid_vali).astype(np.int)\n",
    "vpid = np.array(pid_vali).astype(np.int)\n",
    "vrid = np.array(rid_vali).astype(np.float)\n",
    "playlists_vali = list(dict.fromkeys(vpid))       #vali DATA\n",
    "users_list_vali = list(dict.fromkeys(vuid))      #vali DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_vali = form_coo_matrix(vrid,vuid,vpid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model to tune best parameters to get maximum NDCG\n",
    "def model_vali():\n",
    "    model = LightFM(loss = 'bpr',learning_rate=LEARNING_RATE,\n",
    "                    learning_schedule=LEARNING_SCHEDULE,\n",
    "                    item_alpha=ITEM_ALPHA,\n",
    "                    user_alpha=USER_ALPHA,\n",
    "                    no_components=NUM_COMPONENTS,\n",
    "                    max_sampled =MAX_SAMPLED,\n",
    "                    rho = RHO,\n",
    "                    k=K_M,\n",
    "                    n=N_M,\n",
    "                    epsilon = EPSILON,\n",
    "                    random_state=RANDOM_SEED )\n",
    "    ndcgg = []\n",
    "    ndict = defaultdict(list)\n",
    "    for k in range (100,200, 20):\n",
    "        model.fit(matrix,epochs = k, num_threads = NUM_THREADS) \n",
    "        ngvali = bpr_LighFM(matrix_vali,playlists_vali,users_list_vali,model)\n",
    "        ndict[ngvali] = k\n",
    "        ndcgg.append(ngvali)\n",
    "    epoch_value = ndict.get(max(ndcgg))    \n",
    "    \n",
    "    return epoch_value, max(ndcgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_epoch_value, max_ndcg_value  = model_vali()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch value observed for maximum NDCG on Validation Dataset:  180\n",
      "NDCG@10 on Validation Dataset:  0.08881777085547068\n"
     ]
    }
   ],
   "source": [
    "print(\"Epoch value observed for maximum NDCG on Validation Dataset: \",Max_epoch_value)\n",
    "print(\"NDCG@10 on Validation Dataset: \",max_ndcg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_test, pid_test, rid_test = fetch_attributes(\"test.txt\")\n",
    "tuid = np.array(uid_test).astype(np.int)\n",
    "tpid = np.array(pid_test).astype(np.int)\n",
    "trid = np.array(rid_test).astype(np.float)\n",
    "playlists = list(dict.fromkeys(tpid))       #TEST DATA\n",
    "users_list = list(dict.fromkeys(tuid))      #TEST DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_test = form_coo_matrix(trid,tuid,tpid)   #test data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(Max_epoch_value):\n",
    "    model = LightFM(loss = 'bpr',learning_rate=LEARNING_RATE,\n",
    "                    learning_schedule=LEARNING_SCHEDULE,\n",
    "                    item_alpha=ITEM_ALPHA,\n",
    "                    user_alpha=USER_ALPHA,\n",
    "                    no_components=NUM_COMPONENTS,\n",
    "                    max_sampled =MAX_SAMPLED,\n",
    "                    rho = RHO,\n",
    "                    k=K_M,\n",
    "                    n=N_M,\n",
    "                    epsilon = EPSILON,\n",
    "                    random_state=RANDOM_SEED )\n",
    "    \n",
    "    model.fit(matrix,epochs = Max_epoch_value, num_threads = 2) \n",
    "    val = bpr_LighFM(matrix_test,playlists,users_list,model)\n",
    "     \n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_final = model_test(Max_epoch_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 on Test Dataset:  0.0906686756712301\n"
     ]
    }
   ],
   "source": [
    "#NDCG@10 on Test data using epoch value observed for maximum NDCG on Validation Data\n",
    "print(\"NDCG@10 on Test Dataset: \",ndcg_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1.A - Did Myself\n",
    "Part 1.B - Referred approach from http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/\n",
    "Part 1. C Did Myself\n",
    "Part2 - Discussed with a classmate on the approach to be implemented.Did myself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
